{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "retrofitting_full_version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RArRJPUeLM0",
        "outputId": "40db61e8-d889-45ab-d026-503f08b98c78"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "\n",
        "# File data.py\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "## variables globales\n",
        "vocab, vect_dict, embed_dict = [], {}, {}\n",
        "\n",
        "def read_data(lang,type='embeds'):\n",
        "    vocab.clear()\n",
        "    vect_dict.clear()\n",
        "    embed_dict.clear()\n",
        "\n",
        "    vectors = {}\n",
        "\n",
        "    if lang == 'fra':\n",
        "        if type != 'embeds':\n",
        "            file = \"rg65_french.txt\"\n",
        "        else:\n",
        "            file = \"vecs100-linear-frwiki\"\n",
        "    else:\n",
        "        if type != 'embeds':\n",
        "            file = \"datasets/ws353.txt\"\n",
        "        else:\n",
        "            file = \"vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\"\n",
        "    with open(file, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            l = line.split(\" \")\n",
        "            if (len(l) == 3):                                                   ## retrait du saut de ligne '\\n' pour le fichier de similarité\n",
        "                n = l[2].split('\\n')\n",
        "                l[2] = float(n[0])\n",
        "                vector = [l[1],l[2]]\n",
        "            else :\n",
        "                del l[len(l)-1]\n",
        "                vector = [float(val) for val in l[2:len(l)]]                        ## récupération des vecteurs du type [mot 2, valeur] ou du type [valeur1, valeur2,..., valeurX] pour les fichiers d'embeddings\n",
        "            #print(len(vector))\n",
        "            if len(vector) == 2:\n",
        "              if l[0] not in vectors.keys():                                    ## ajout des vecteurs dans un dictionnaire avec le mot 1 en clé et le vecteur en valeur\n",
        "                  vocab.append(l[0])\n",
        "                  vectors[l[0]] = vector\n",
        "                  if l[1] not in vectors.keys():                                ## ajout des vecteurs dans un dictionnaire avec le mot 2 en clé et le vecteur [mot 1, valeur] en valeur\n",
        "                    vocab.append(l[1])\n",
        "                    vectors[l[1]] = [[l[0],float(l[2])]]\n",
        "                  else :\n",
        "                    vectors[l[1]].append([l[0],float(l[2])])\n",
        "              else :\n",
        "                  vectors[l[0]].append(vector)\n",
        "                  if l[1] not in vectors.keys():                                ## ajout des vecteurs dans un dictionnaire avec le mot 2 en clé et le vecteur [mot 1, valeur] en valeur\n",
        "                    vectors[l[1]] = [[l[0],float(l[2])]]\n",
        "                  else :\n",
        "                    vectors[l[1]].append([l[0],float(l[2])])\n",
        "            else :\n",
        "              if l[0] not in vectors.keys():                                    ## ajout des vecteurs dans un dictionnaire avec le mot 1 en clé et le vecteur en valeur\n",
        "                  vocab.append(l[0])\n",
        "                  vectors[l[0]] = vector\n",
        "              else :\n",
        "                  vectors[l[0]].append(vector)\n",
        "\n",
        "    return vectors,vocab\n",
        "\n",
        "\n",
        "\n",
        "similarity_dict, similarity_vocab = read_data(\"fra\",\"vect\")                          ## dictionnaire pour la similarité cosinus\n",
        "\n",
        "#print(vect_dict['corde'])\n",
        "#print(vect_dict)\n",
        "#print(\"VECTS\",len(vect_dict))\n",
        "\n",
        "embeddings_dict, embeddings_vocab = read_data(\"fra\")                ## dictionnaire pour le retrofitting et la tâche d'analyse de sentiments\n",
        "#print(\"corde : \", embed_dict['corde'])\n",
        "#print(\"la : \", embed_dict['la'])\n",
        "#print(\"EMBEDS\",len(embed_dict))\n",
        "\n",
        "def find_vector(word,dic):\n",
        "    ## Fonction qui permet de récupérer le vecteur d'un certain mot\n",
        "    ## return [mot 2, value] pour le cas du dictionnaire vect_dict\n",
        "    ## return [embeddings] pour le cas du dictionnaire embed_dict\n",
        "    for key, value in dic.items():\n",
        "        if word == key:\n",
        "            return value\n",
        "    return(\"Le mot n'a pas été trouvé dans le lexique.\")\n",
        "\n",
        "#print(find_vector(\"corde\", embed_dict))\n",
        "#print(find_vector(\"idk\", embed_dict))\n",
        "\n",
        "\n",
        "vocabulary = set(embeddings_vocab + similarity_vocab)\n",
        "vocabulary = list(vocabulary)\n",
        "print(len(vocabulary))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17481\n",
            "17481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "CXExtSvaeSLJ",
        "outputId": "238fc4a9-7e4e-40ff-8bdb-0a0c4cc98bfe"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "\n",
        "## File retrofitting.py\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('omw')\n",
        "nltk.download('wordnet')  # utilisation de WOLF via NLTK wordnet\n",
        "\n",
        "#from data import similarity_dict, embeddings_dict, vocabulary\n",
        "\n",
        "def get_synsets(word,lang):\n",
        "    ## Méthode pour récupérer tous les mots en relation avec celui donné en argument\n",
        "    return wn.synsets(word,lang=lang)\n",
        "\n",
        "\n",
        "def lemma(synsets,lang):\n",
        "    ## Méthode pour récupérer uniquement les mots des synsets et pas 'word.n.01' par exemple\n",
        "    lemmas,list_lemmas = [],[]\n",
        "    for synset in synsets:\n",
        "        lemmas = synset.lemma_names(lang)\n",
        "        for lemma in lemmas:\n",
        "            list_lemmas.append(lemma)\n",
        "    return list_lemmas\n",
        "\n",
        "\n",
        "def get_hypernyms(word,lang):\n",
        "    ## Méthode pour récupérer tous les mots en relation d'hypernymie avec celui donnée en argument\n",
        "    synsets = get_synsets(word,lang=lang)\n",
        "    hyp = []\n",
        "    if synsets != [] :\n",
        "        for synset in synsets:\n",
        "            hyp += synset.hypernyms()\n",
        "    return hyp\n",
        "\n",
        "\n",
        "def get_hyponyms(word,lang):\n",
        "    ## Méthode pour récupérer tous les mots en relation d'hyponymie avec celui donnée en argument\n",
        "    synsets = get_synsets(word,lang=lang)\n",
        "    hyp = []\n",
        "    if synsets != [] :\n",
        "        for synset in synsets:\n",
        "            hyp += synset.hyponyms()\n",
        "    return hyp\n",
        "\n",
        "\n",
        "def neighbors(word,lang,rel='neighb',list_neighb=[]):\n",
        "    ## Méthode pour récupérer les voisins d'un certain mot en fonction du type de relation donnée en argument ou\n",
        "    list_neighb = []\n",
        "    # Récupération des synsets avec le type de relation précisé ou non\n",
        "    if rel == 'hyponym':\n",
        "        synsets = get_hyponyms(word,lang)\n",
        "    elif rel == 'hypernym':\n",
        "        synsets = get_hypernyms(word,lang)\n",
        "    elif rel == 'synonym':\n",
        "        synsets = get_synonyms(word,lang)\n",
        "    else:\n",
        "        synsets = get_synsets(word,lang)\n",
        "\n",
        "    # Ajout de la liste de tous les mots appartenant au synset à un dictionnaire dont la clé est le mot donné en argument et la valeur est une liste de mots voisins\n",
        "    for synset in synsets:\n",
        "        if (synset not in list_neighb) or (list_neighb == []):\n",
        "            if synset in vocab:\n",
        "                list_neighb.append(synset.lemma_names(lang))\n",
        "    return list_neighb\n",
        "\n",
        "\n",
        "def retrofit(num_iter,vocab,word_dict,lang,relation='neighb'):\n",
        "    ## Fonction de retrofitting\n",
        "    ## D'après l'algorithme de Faruqui\n",
        "    vocabulary = vocab.intersection(set(word_dict.keys()))\n",
        "    vectors_dict = word_dict\n",
        "\n",
        "    for iter in range(num_iter):\n",
        "\n",
        "        for word in vocabulary:\n",
        "            if word in vectors_dict.keys():\n",
        "                word_vect = vectors_dict[word]\n",
        "            else : word_vect = []\n",
        "\n",
        "            list_neighb = neighbors(word,lang,relation)\n",
        "            num_neighb = len(list_neighb)\n",
        "\n",
        "            if list_neighb != []:\n",
        "                word_vect = word_dict[word] * num_neighb\n",
        "                for neighb in list_neighb:\n",
        "                    if neighb in vectors_dict.keys():\n",
        "                        word_vect += vectors_dict[neighb]\n",
        "                        #print(\"word vect 2\",word_vect)\n",
        "                vectors_dict[word] = word_vect/(2*num_neighb)\n",
        "\n",
        "    print(\"DONE\")\n",
        "    return vectors_dict\n",
        "\n",
        "\"\"\"\n",
        "BATCH_SIZE = 5\n",
        "vocab_list = list(vocab)\n",
        "new_vectors = {}\n",
        "\n",
        "for i in range(5):\n",
        "    batch = vocab_list[i:BATCH_SIZE]\n",
        "    print(batch)\n",
        "    set_batch = set(batch)\n",
        "    new_vectors.update(retrofit(5,set_batch,embed_dict,'fra'))\n",
        "print(\"TEST AVANT RETROFIT\",batch[0],embed_dict[batch[0]])\n",
        "print(\"TEST APRES RETROFIT\",batch[0],new_vectors[batch[0]])\n",
        "\"\"\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Package omw is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nBATCH_SIZE = 5\\nvocab_list = list(vocab)\\nnew_vectors = {}\\n\\nfor i in range(5):\\n    batch = vocab_list[i:BATCH_SIZE]\\n    print(batch)\\n    set_batch = set(batch)\\n    new_vectors.update(retrofit(5,set_batch,embed_dict,\\'fra\\'))\\nprint(\"TEST AVANT RETROFIT\",batch[0],embed_dict[batch[0]])\\nprint(\"TEST APRES RETROFIT\",batch[0],new_vectors[batch[0]])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdyi_uc6QHA-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7cd808f4-cac2-44d6-9f4d-e87bc316597e"
      },
      "source": [
        "# File movies_critics.py\n",
        "\n",
        "try:\n",
        "  import allocine\n",
        "except ImportError:\n",
        "  !pip install allocine-wrapper  ## Pas sûres d'utiliser cette librairie\n",
        "\n",
        "import re\n",
        "import csv\n",
        "import torch\n",
        "import numpy as np\n",
        "import sklearn \n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from keras import *\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "embed_size = len(embeddings_dict[\"and\"])  ## récupération du nombre de features d'un mot du vocabulaire pour créer la matrice d'embeddings\n",
        "\n",
        "def get_data_eng_sentiment(typ):\n",
        "  ## récupération des données pour l'analyse de sentiments en anglais\n",
        "  ## fichiers de stanford\n",
        "  if typ == \"train\" :\n",
        "    file = \"stanford_raw_train.txt\"\n",
        "  elif typ == \"dev\":\n",
        "    file = \"stanford_raw_dev.txt\"\n",
        "  else:\n",
        "    file = \"stanford_raw_test.txt\"\n",
        "  critics = []\n",
        "\n",
        "  with open(file, encoding='utf-8') as f:\n",
        "    corpus_vocab = []\n",
        "    for line in f:\n",
        "      ## chaque ligne de type \"1 | -1\" + \"text\"\n",
        "      if re.match(r'\\d\\s[A-Za-z]+', line):\n",
        "        l = line.split(\" \")\n",
        "        #print(l)\n",
        "        sentiment = l[0]\n",
        "        text = []\n",
        "        for i in range(1,len(l)):\n",
        "          text.append(l[i])\n",
        "        critics.append((sentiment,\" \".join(text)))\n",
        "  return critics\n",
        "\n",
        "def get_data_fra_sentiment(typ=\"train\"):\n",
        "  ## récupération des données pour l'analyse de sentiments en français\n",
        "  ## fichiers de allociné, corpus créé par Théophile Blard\n",
        "    file_table = []\n",
        "    if typ == \"train\" :\n",
        "        file = \"train.csv\"\n",
        "    elif typ == \"dev\":\n",
        "        file = \"valid.csv\"\n",
        "    else:\n",
        "        file = \"test.csv\"\n",
        "    critics = []\n",
        "\n",
        "    with open(file, newline='') as f:\n",
        "      reader = csv.reader(f)\n",
        "      for row in reader:\n",
        "          file_table.append(row)\n",
        "      f.close()\n",
        "\n",
        "    for i in range(len(file_table)):\n",
        "      if file_table[2] == 0:\n",
        "        file_table[2] = -1\n",
        "      critics.append((file_table[1],file_table[2]))\n",
        "    return critics\n",
        "\n",
        "\n",
        "def fit_data(critics):\n",
        "  ## fonction pour que le modèle soit entraîné sur les données \n",
        "  X,Y = [],[]\n",
        "  for sent,text in critics :\n",
        "    X.append(text)\n",
        "    Y.append(sent)\n",
        "\n",
        "  tokenizer = Tokenizer(lower=True,split=' ')\n",
        "  tokenizer.fit_on_texts(X)\n",
        "  X = tokenizer.texts_to_sequences(X)\n",
        "  X = pad_sequences(X)\n",
        "  return X,Y\n",
        "\n",
        "def get_embedding_mat(embed_dict,corpus_vocab):\n",
        "  ## Fonction de création de matrice d'embeddings\n",
        "  matrix = np.zeros((len(corpus_vocab),embed_size))\n",
        "  #print(matrix.shape) # (10538, 99)\n",
        "  for i,word in enumerate(corpus_vocab):\n",
        "    vector = [0]*embed_size\n",
        "    if (word in embed_dict.keys()) and (len(embed_dict[word]) == embed_size):\n",
        "      ## On ne récupère que les vecteurs des mots qui ont la bonne taille et qui font partie du dictionnaire des valeurs pré entrainées\n",
        "      vector = embed_dict[word]\n",
        "      #print(len(vector))\n",
        "    matrix[i] = vector\n",
        "  return matrix\n",
        "\n",
        "embedding_matrix = get_embedding_mat(embeddings_dict,vocabulary)\n",
        "\n",
        "train_critics_eng = get_data_eng_sentiment(\"train\")\n",
        "X_train, Y_train = fit_data(train_critics_eng) \n",
        "\n",
        "dev_critics_eng = get_data_eng_sentiment(\"dev\")\n",
        "X_dev, Y_dev = fit_data(dev_critics_eng) \n",
        "\n",
        "train_critics_fra = get_data_fra_sentiment(\"train\")\n",
        "X_train_f, Y_train_f = fit_data(train_critics_fra) \n",
        "\n",
        "# MLP avec sklearn\n",
        "MLP_model = MLPClassifier(hidden_layer_sizes=(100,),activation='tanh',alpha=0.001,solver='adam',max_iter=5000,n_iter_no_change=5)\n",
        "\n",
        "\n",
        "def fit_and_predict(X,Y):\n",
        "    MLP_model.fit(X,Y)\n",
        "    print(\"preds : \",MLP_model.predict(X))\n",
        "    print(\"preds probas: \",MLP_model.predict_proba(X))\n",
        "    print(\"score : \",MLP_model.score(X,Y))\n",
        "    print(\"accuracy : \",accuracy_score(Y,MLP_model.predict(X)))\n",
        "    print(\"loss : \",MLP_model.loss_)\n",
        "\n",
        "fit_and_predict(X_dev,Y_dev)\n",
        "\n",
        "# MLP avec Keras\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary), embed_size, weights=[embedding_matrix])) \n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4780c4960ed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMLP_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mget_info_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# MLP avec Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-4780c4960ed4>\u001b[0m in \u001b[0;36mget_info_model\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_info_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preds : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMLP_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preds probas: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMLP_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMLP_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    969\u001b[0m         \"\"\"\n\u001b[1;32m    970\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    683\u001b[0m                                          layer_units[i + 1])))\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# forward propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m--> 104\u001b[0;31m                                                  self.coefs_[i])\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 47 is different from 44)"
          ]
        }
      ]
    }
  ]
}